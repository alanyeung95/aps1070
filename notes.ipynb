{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeLveYVW2V6/AB0pfp1IXX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanyeung95/aps1070/blob/main/notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## k-fold cross-validation\n",
        "\n",
        "\n",
        "In k-fold cross-validation, training data is divided into k folds, then the model is trained on k-1 folds and validated on 1 fold, for a total of k times (splits)\n",
        "\n",
        "ref: https://scikit-learn.org/stable/modules/cross_validation.html"
      ],
      "metadata": {
        "id": "EPEutA-PU8ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## normal vector"
      ],
      "metadata": {
        "id": "VaE6j2Jw1QpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "Image(url= \"https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/vector-norms-10.png\",  width=300, height=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "uYAax4_UzqgE",
        "outputId": "4c473227-12ed-41bf-f389-add2e0ff269e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/vector-norms-10.png\" width=\"300\" height=\"300\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## orthonormal basis\n",
        "\n",
        "\n",
        "We say that 2 vectors are orthogonal if they are perpendicular to each other. i.e. the dot product of the two vectors is zero. \n",
        "\n",
        "### 2d vector dot product\n",
        "v1.x * v2.y - v1.y * v2.*x*\n",
        "\n",
        "### ref\n",
        "https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthonormal-basis/v/linear-algebra-introduction-to-orthonormal-bases#:~:text=An%20orthonormal%20basis%20is%20a,is%20called%20the%20kronecker%20delta.\n"
      ],
      "metadata": {
        "id": "i6lg1M0V693I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inverse of matrix \n",
        "\n",
        "### detect inverse matrix\n",
        "\n",
        "\n",
        "determinant of these matrices is non-zero.\n",
        "\n",
        "matrix B \n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "[[1,-4,2],\n",
        "[-2,,3],\n",
        "[2,6,8]]\n",
        "\n",
        "|A| = a(ei ‚àí fh) ‚àí b(di ‚àí fg) + c(dh ‚àí eg)\n",
        "\n",
        "det A = 1 (8 - 18) + 4 (-16 - 6) + 2(-12 - 2) = -126 ‚â† 0.\n",
        "\n",
        "ref: https://www.mathsisfun.com/algebra/matrix-determinant.html"
      ],
      "metadata": {
        "id": "auWPUuerH_zT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using gaussian elimination"
      ],
      "metadata": {
        "id": "uMfyToEPerlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#<img src=\"diagrams/diagram-1.png\" alt=\"drawing\" width=\"400\"/>"
      ],
      "metadata": {
        "id": "Xl697vZlG-_V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0FKWUG1QQqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Find the Angle Between Two Vectors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P4VvK9wzejT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url= \"https://mathsathome.com/wp-content/uploads/2021/12/how-to-find-the-angle-between-two-vectors.png\",  width=600, height=300)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "tOvCyORYQFwq",
        "outputId": "1fa5bada-95de-4558-d69a-d779a1913ad4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://mathsathome.com/wp-content/uploads/2021/12/how-to-find-the-angle-between-two-vectors.png\" width=\"600\" height=\"300\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability density functions"
      ],
      "metadata": {
        "id": "EpqrGfE1Cwd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## true positive vs false positive, roc \n",
        "https://scottroy.github.io/ROC-space-and-AUC.html"
      ],
      "metadata": {
        "id": "1FgwUNIBC4vT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixture of Gaussians "
      ],
      "metadata": {
        "id": "5p40e_-RC0-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## eigenvectors and eigenvalues\n",
        "The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.\n",
        "\n",
        "Methods to perform PCA:\n",
        "\n",
        "The classic approach to PCA is to perform the eigendecomposition on the covariance matrix Œ£, which is a ùëë√óùëë matrix where each element represents the covariance between two features.\n",
        "But, most PCA implementations perform a Singular Vector Decomposition (SVD) to improve the computational efficiency.\n",
        "\n",
        "### why convariance for PCA\n",
        "Let's say you want to reduce the dimensionality of your data set, say down to just one dimension. In general, this means picking a unit vector u, and replacing each data point, xi, with its projection along this vector, uTxi. Of course, you should choose u so that you retain as much of the variation of the data points as possible: if your data points lay along a line and you picked u orthogonal to that line, all the data points would project onto the same value, and you would lose almost all the information in the data set! So you would like to maximize the variance of the new data values uTxi.\n",
        "\n",
        "ref:„ÄÄhttps://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component"
      ],
      "metadata": {
        "id": "uyC0lfvONTFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Singular Value Decomposition\n",
        "Singular Value Decomposition (SVD) is a widely used technique to decompose a matrix into several component matrices, exposing many of the useful and interesting properties of the original matrix.\n",
        "\n",
        "ref:„ÄÄhttps://www.geeksforgeeks.org/singular-value-decomposition-svd/\n",
        "\n",
        "ref: https://www.d.umn.edu/~mhampton/m4326svd_example.pdf\n"
      ],
      "metadata": {
        "id": "TgiC5P5eT1O8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mini-batch gradient descent"
      ],
      "metadata": {
        "id": "X8OCY52ViO5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "D = (ùíô(1), ùë¶(1)), ‚Ä¶ (ùíô(ùëÅ), ùë¶(ùëÅ))\n",
        "D_train_x, D_train_y, D_test_x, D_test_y = split_dataset(D)\n",
        "iteration_step = T\n",
        "learning_rate = a_0\n",
        "learning_rate_decay = a_d\n",
        "weights = w \n",
        "sizeOfInput = N\n",
        "\n",
        "for epoch in number of epochs:\n",
        "    for batch in m:\n",
        "        pred = prediction(D_train_x, w)\n",
        "        error = calculateError(pred, D_train_y)\n",
        "        w = w - (learning_rate/sizeOfInput)*(error)\n",
        "    learning_rate  = learning_rate  * learning_rate_decay\n",
        "```\n"
      ],
      "metadata": {
        "id": "cdcUP2tMiFpu"
      }
    }
  ]
}